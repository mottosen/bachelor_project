{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torchio as tio\n",
    "\n",
    "# common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data import detection_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = Path(\"../data/detectron2/axial\")\n",
    "data_path = Path(\"../data/synthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_board_dicts(imgdir):\n",
    "    json_file = imgdir / \"dataset.json\"\n",
    "    \n",
    "    with open(json_file) as f:\n",
    "        dataset_dicts = json.load(f)\n",
    "\n",
    "    for i in dataset_dicts:\n",
    "        filename = i[\"file_name\"] \n",
    "        i[\"file_name\"] = imgdir / filename \n",
    "        i[\"width\"] = int(i[\"width\"])\n",
    "        i[\"height\"] = int(i[\"height\"])\n",
    "\n",
    "        for j in i[\"annotations\"]:\n",
    "            j[\"bbox\"] = [float(num) for num in j[\"bbox\"]]\n",
    "            j[\"bbox_mode\"] = int(j[\"bbox_mode\"]) # BoxMode.XYWH_ABS\n",
    "            j[\"category_id\"] = int(j[\"category_id\"])\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "def mapper(dataset_dict):\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)\n",
    "\n",
    "    image = np.load(dataset_dict[\"file_name\"])\n",
    "\n",
    "    auginput = T.AugInput(image)\n",
    "    image = torch.from_numpy(auginput.image.transpose(2, 0, 1))\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(annotation, [], image.shape[1:])\n",
    "        for annotation in dataset_dict.pop(\"annotations\")\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "       \"image\": image,\n",
    "       \"image_id\": dataset_dict[\"image_id\"],\n",
    "       \"width\": dataset_dict[\"width\"],\n",
    "       \"height\": dataset_dict[\"height\"],\n",
    "       \"instances\": utils.annotations_to_instances(annos, image.shape[1:])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/detectron2/blob/main/projects/DeepLab/train_net.py\n",
    "\n",
    "class SyntheticTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return detectron2.data.build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        return detectron2.data.build_detection_test_loader(cfg, dataset_name, mapper=mapper)\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name):\n",
    "        return COCOEvaluator(dataset_name, output_dir=cfg.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the data sets\n",
    "\n",
    "def register_data(set_name, set_path):\n",
    "    DatasetCatalog.register(\"brain_metastasis_\" + set_name, lambda d=set_path: get_board_dicts(d))\n",
    "    MetadataCatalog.get(\"brain_metastasis_\" + set_name).set(thing_classes=[\"TUMOR\"])\n",
    "\n",
    "register_data('train', data_path / 'train')\n",
    "register_data('test', data_path / 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"brain_metastasis_train\",)\n",
    "cfg.DATASETS.TEST = (\"brain_metastasis_test\",)\n",
    "\n",
    "# Number of data loading threads\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "#cfg.MODEL.WEIGHTS = os.path.join('./output_first_model', \"model_final.pth\")  # weights from model we just trained\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # weights from detectron2 model zoo\n",
    "\n",
    "# Number of images per batch across all machines.\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.01\n",
    "cfg.SOLVER.MAX_ITER = 1000\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "\n",
    "cfg.OUTPUT_DIR = './output_second_model'\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "trainer = SyntheticTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing predictions\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"./output_synthetic_bnu/model_final.pth\"\n",
    "cfg.DATASETS.TEST = (\"brain_metastasis_test\", )\n",
    "predictor = DefaultPredictor(cfg)\n",
    "test_metadata = MetadataCatalog.get(\"brain_metastasis_test\")\n",
    "\n",
    "limit = 1\n",
    "\n",
    "for imageName in data_path.glob('test/*'):\n",
    "  if limit == 0:\n",
    "    break\n",
    "  limit -= 1\n",
    "  im = np.load(imageName)\n",
    "  outputs = predictor(im)\n",
    "  v = Visualizer(im[:, :, ::-1],\n",
    "                metadata=test_metadata, \n",
    "                scale=0.8\n",
    "                 )\n",
    "  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "  cv2.imwrite(f\"./inference{limit}.png\", out.get_image()[:, :, ::-1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c54106a16a951d6a5017109f04d641d7c3fbff9718dc9b5fc0eafbd427481a36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('synthetic_data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
